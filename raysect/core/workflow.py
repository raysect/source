# Copyright (c) 2014-2025, Dr Alex Meakins, Raysect Project
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
#     1. Redistributions of source code must retain the above copyright notice,
#        this list of conditions and the following disclaimer.
#
#     2. Redistributions in binary form must reproduce the above copyright
#        notice, this list of conditions and the following disclaimer in the
#        documentation and/or other materials provided with the distribution.
#
#     3. Neither the name of the Raysect Project nor the names of its
#        contributors may be used to endorse or promote products derived from
#        this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
# POSSIBILITY OF SUCH DAMAGE.

from collections import defaultdict
from multiprocessing import get_context, cpu_count
import platform
import os
import subprocess
from raysect.core.math import random
import time

try:
    from mpi4py import MPI
    HAVE_MPI = True
except ImportError:
    HAVE_MPI = False


class RenderEngine:
    """
    Provides a common rendering workflow interface.

    This is a base class, its functionality must be implemented fully by the deriving class.

    This class provides a rendering workflow that abstracts away the underlying
    system performing the work. It is intended that render engines may be built
    that provide rendering on single cores, multi-cores (SMP) and clusters.

    The basic workflow is as follows. The render task is split into small,
    self-contained chunks of work - 'tasks'. These tasks are passed to the
    render engine which distributes the work to the available computing
    resources. These discrete computing resources are know as "workers".
    Workers process one task at a time and return their result to the render
    engine. When results are received the render engine assembles them into
    the final result.

    This workflow is implemented by supplying a set of tasks and two methods to
    the render engines' run() method which processes those tasks. The functions
    supplied to the run() method may be given additional args and kwargs.

    A worker calls render for each task object received. render has the following signature: ::

        def render(task, *render_args, **render_kwargs)

    where args and kwargs are additional arguments supplied by the user.

    Similarly, the worker calls update() for the results generated by a call to
    render(). Update() has the following signature: ::

        def update(results, *update_args, **update_kwargs)

    where args and kwargs are additional arguments supplied by the user.

    The render() function must return an object representing the results,
    this must be a picklable python object.

    The execution order of tasks is not guaranteed to be in order. If the order
    is critical, an identifier should be passed as part of the task definition
    and returned in the result. This will permit the order to be reconstructed.
    """

    def run(self, tasks, render, update, render_args=(), render_kwargs={}, update_args=(), update_kwargs={}):
        """
        Starts the render engine executing the requested tasks.

        :param list tasks: List of user defined tuples that describe the task to execute.
        :param object render: Callable python object that executes the tasks.
        :param object update: Callable python object that is called following a render task and must be
          used to update the internal state of the object requesting work.
        :param tuple render_args: Additional arguments to pass to user defined render function.
        :param tuple render_kwargs: Additional keyword arguments to pass to user defined render function.
        :param tuple update_args: Additional arguments to pass to user defined update function.
        :param tuple update_kwargs: Additional keyword arguments to pass to user defined update function.
        """
        raise NotImplementedError("Virtual method must be implemented in sub-class.")

    def worker_count(self):
        """
        Returns the number of workers in use by this engine.
        """
        raise NotImplementedError("Virtual method must be implemented in sub-class.")


class SerialEngine(RenderEngine):
    """
    Render engine for running on a single CPU processor.

    This engine is useful for debugging.

        >>> from raysect.core import SerialEngine
        >>> from raysect.optical.observer import PinholeCamera
        >>>
        >>> camera = PinholeCamera((512, 512))
        >>> camera.render_engine = SerialEngine()
    """

    def run(self, tasks, render, update, render_args=(), render_kwargs={}, update_args=(), update_kwargs={}):

        for task in tasks:
            result = render(task, *render_args, **render_kwargs)
            update(result, *update_args, **update_kwargs)

    def worker_count(self):
        return 1


class MulticoreEngine(RenderEngine):
    """
    A render engine for distributing work across multiple CPU cores.

    The number of processes spawned by this render engine is controlled via
    the processes attribute. This can also be set at object initialisation.
   
    If the processes attribute is set to None (the default), the render engine
    will automatically set the number of processes to be equal to the number
    of CPU cores detected on the machine.

    If a render is being performed where the time to compute an individual task
    is comparable to the latency of the inter process communication (IPC), the
    render may run significantly slower than expected due to waiting for the
    IPC to complete. To reduce the impact of the IPC overhead, multiple tasks
    are grouped together into jobs, requiring only one IPC wait for multiple
    tasks.

    By default the number of tasks per job is adjusted automatically. The
    tasks_per_job attribute can be used to override this automatic adjustment.
    To reenable the automated adjustment, set the tasks_per_job attribute to
    None.

    :param processes: The number of worker processes, or None to use all available cores (default).
    :param tasks_per_job: The number of tasks to group into a single job, or None if this should be determined automatically (default).
    :param start_method: The method used to start child processes: 'fork' (default), 'spawn' or 'forkserver'.

    .. code-block:: pycon

        >>> from raysect.core import MulticoreEngine
        >>> from raysect.optical.observer import PinholeCamera
        >>>
        >>> camera = PinholeCamera((512, 512))
        >>>
        >>> # allowing the camera to use all available CPU cores.
        >>> camera.render_engine = MulticoreEngine()
        >>>
        >>> # or forcing the render engine to use a specific number of CPU processes
        >>> camera.render_engine = MulticoreEngine(processes=8)
    """

    def __init__(self, processes=None, tasks_per_job=None, start_method='fork'):
        super().__init__()
        self.processes = processes
        self.tasks_per_job = tasks_per_job
        self._context = get_context(start_method)

    @property
    def processes(self):
        return self._processes

    @processes.setter
    def processes(self, value):
        if value is None:
            self._processes = cpu_count()
        else:
            value = int(value)
            if value <= 0:
                raise ValueError('Number of concurrent worker processes must be greater than zero.')
            self._processes = value

    @property
    def tasks_per_job(self):
        return self._tasks_per_job

    @tasks_per_job.setter
    def tasks_per_job(self, value):
        if value is None:
            self._tasks_per_job = 1
            self._auto_tasks_per_job = True
        else:
            if value < 1:
                raise ValueError("The number of tasks per job must be greater than zero or None.")
            self._tasks_per_job = value
            self._auto_tasks_per_job = False

    def run(self, tasks, render, update, render_args=(), render_kwargs={}, update_args=(), update_kwargs={}):

        # establish ipc queues
        job_queue = self._context.SimpleQueue()
        result_queue = self._context.SimpleQueue()
        tasks_per_job = self._context.Value('i')

        # start process to generate jobs
        tasks_per_job.value = self._tasks_per_job
        producer = self._context.Process(target=self._producer, args=(tasks, job_queue, tasks_per_job))
        producer.start()

        # start worker processes
        workers = []
        for pid in range(self._processes):
            p = self._context.Process(target=self._worker, args=(render, render_args, render_kwargs, job_queue, result_queue))
            p.start()
            workers.append(p)

        # consume results
        remaining = len(tasks)
        while remaining:

            results = result_queue.get()

            # has a worker failed?
            if isinstance(results, Exception):

                # clean up
                for worker in workers:
                    if worker.is_alive():
                        worker.terminate()
                producer.terminate()

                # wait for processes to terminate
                for worker in workers:
                    worker.join()
                producer.join()

                # raise the exception to inform the user
                raise results

            # update state with new results
            for result in results:
                update(result, *update_args, **update_kwargs)
                remaining -= 1

        # shutdown workers
        for _ in workers:
            job_queue.put(None)

        # store tasks per job value for next run
        self._tasks_per_job = tasks_per_job.value

    def worker_count(self):
        return self._processes

    def _producer(self, tasks, job_queue, stored_tasks_per_job):

        # initialise request rate controller constants
        target_rate = 50  # requests per second
        min_time = 1      # seconds
        min_requests = min(2 * target_rate, 5 * self._processes)
        tasks_per_job = stored_tasks_per_job.value

        # split tasks into jobs and dispatch to workers
        requests = -self.processes  # ignore the initial jobs, the requests are instantaneous
        start_time = time.time()
        while tasks:

            # assemble job
            job = []
            for _ in range(tasks_per_job):
                if tasks:
                    job.append(tasks.pop())
                    continue
                break

            # add job to queue
            job_queue.put(job)
            requests += 1

            # if enabled, auto adjust tasks per job to keep target requests per second
            if self._auto_tasks_per_job:

                elapsed_time = (time.time() - start_time)
                if elapsed_time > min_time and requests > min_requests:

                    # re-normalise the tasks per job based on previous work to propose a new value
                    requests_rate = requests / elapsed_time
                    proposed = tasks_per_job * requests_rate / target_rate

                    # gradually adjust tasks per job to reduce risk of oscillation
                    tasks_per_job = 0.1 * proposed + 0.9 * tasks_per_job
                    tasks_per_job = max(1, round(tasks_per_job))

                    # reset counters
                    requests = 0
                    start_time = time.time()

        # pass back new value
        stored_tasks_per_job.value = tasks_per_job

    def _worker(self, render, args, kwargs, job_queue, result_queue):

        # re-seed the random number generator to prevent all workers inheriting the same sequence
        random.seed()

        # process jobs
        while True:

            job = job_queue.get()

            # have we been commanded to shutdown?
            if job is None:
                break

            results = []
            for task in job:
                try:
                    results.append(render(task, *args, **kwargs))
                except Exception as e:
                    # pass the exception back to the main process and quit
                    result_queue.put(e)
                    break

            # hand back results
            result_queue.put(results)


class MPIEngine(RenderEngine):
    """
    Render engine for running in an MPI context.

    This engine is useful for distributed memory systems, and for shared
    memory systems where the overhead of inter-process communication of
    the scenegraph is large compared with the time taken for a single
    process to produce the scenegraph (e.g. on Windows).

    This render engine requires mpi4py to be installed, and the program
    to be run using ``mpirun -n <nproc> python <script.py>``.

        >>> from raysect.core import MPIEngine
        >>> from raysect.optical.observer import PinholeCamera
        >>>
        >>> camera = PinholeCamera((512, 512))
        >>> camera.render_engine = MPIEngine()

    The render engine uses the single process, multiple data (SPMD) paradigm.
    Each process is treated as a separate serial renderer. It is assumed that
    the scene graph is created in every process, and each process processes a
    subset of the total rendering tasks sequentially. The results are then
    gathered back to the root (rank 0) process.

    The SPMD paradigm means there are many copies of the program running and
    each copy runs the same instructions, so programs must be written with
    this in mind. While all copies build the scene to render, only one copy
    (the process with rank 0) actually receives all the render results
    and calls the update function. This means that after an observe,
    only the rank 0 process contains the results of the call to the render
    function for all tasks, and so any post processing (including plotting
    or saving images) should only be done on the rank 0 process.

    Also, any further renders which depend on the results of a previous render
    (for example, using adapive samplers) will require communication of the
    results from rank 0 to all the other processes:

        >>> pipeline = RGBPipeline2d()
        >>> camera.pipelines = [pipeline]
        >>> camera.sampler = RGBAdaptivesampler2d(pipeline)
        >>> camera.observe()
        >>> # Update the sampler in each worker process with the results
        >>> # of the previous render for the next pass.
        >>> root_pipeline = camera.render_engine.comm.bcast(pipeline, root=0)
        >>> camera.sampler.pipeline = root_pipeline
        >>> # Now subsequent observes will have the correct statistics.
        >>> camera.observe()

    The class contains some attributes relevant to the MPI environment:

    :ivar comm: the MPI communicator (``MPI_COMM_WORLD``).
    :ivar rank: the process rank in the communicator. Only rank 0 contains the
                full results after a call ``to RenderEngine.run()``.
    :ivar size: the number of processes in the communicator.
    """
    def __init__(self):
        if not HAVE_MPI:
            raise RuntimeError("The mpi4py package is required to use this engine.")
        comm = MPI.COMM_WORLD
        self.comm = comm
        self.rank = comm.Get_rank()
        self.size = comm.Get_size()
        if self.size < 2:
            raise RuntimeError("At least 2 separate processes are required to use this engine.")

    def run(self, tasks, render, update, render_args=(), render_kwargs=None, update_args=(), update_kwargs=None):
        # Avoid mutable default arguments.
        if render_kwargs is None:
            render_kwargs = {}
        if update_kwargs is None:
            update_kwargs = {}
        # All processes must have the same tasks, in the same order, so that
        # all processes agree on which subset of tasks each is to work on.
        tasks = self.comm.bcast(tasks, root=0)
        ntasks = len(tasks)
        nworkers = self.size - 1
        worker_tasks = defaultdict(list)
        for i, task in enumerate(tasks):
            worker_tasks[i % nworkers].append(task)
        if self.rank == 0:  # The root node processes the results.
            remaining = ntasks
            while remaining:
                result = self.comm.recv()
                if isinstance(result, Exception):
                    raise result
                update(result, *update_args, **update_kwargs)
                remaining -= 1
        else:  # Each worker renders the subset of tasks assigned to them.
            # Unlike MulticoreEngine, there is no need to re-seed the random
            # number generator to prevent all workers inheriting the same sequence
            # since all processes are independent.
            for task in worker_tasks[self.rank - 1]:
                try:
                    result = render(task, *render_args, **render_kwargs)
                except Exception as e:
                    result = e
                self.comm.send(result, 0)
        self.comm.Barrier()

    def worker_count(self):
        return self.size


class HybridEngine(RenderEngine):
    """
    Render engine for combined shared and distributed memory systems.

    This render engine requires mpi4py to be installed, and the program
    to be run using ``mpirun -n <ntasks> python <script.py>``.

        >>> from raysect.core import HybridEngine, MulticoreEngine
        >>> from raysect.optical.observer import PinholeCamera
        >>>
        >>> nworkers = 4
        >>> camera = PinholeCamera((512, 512))
        >>> camera.render_engine = HybridEngine(MulticoreEngine(nworkers))

    The render engine uses a variation of the single process, multiple
    data (SPMD) paradigm.  In this paradigm, "tasks" are independent
    processes potentially running on separate computers. Each task will
    have one or more "workers" which it spawns in order to do its share
    of the rendering. Each worker will be on the same computer as its
    parent task and should share the parent task's memory.

    When the program is started with `mpirun`, there will be `ntasks`
    separate processes, all of which run concurrently and build their
    own copies of the scenegraph. Then when this render engine's `run`
    method is called in each process, the sub-engine will create
    `nworkers` worker subprograms to perform a subset of the render: how
    this is done depends entirely on which sub-engine is used. For
    example, if the `MulticoreEngine` is used then `nworkers`
    subprocesses will be forked from the parent task to perform the
    render subset. If the `SerialEngine` is used then the render subset
    will be computed in serial in the task's own process. The rendering
    results for all workers in all tasks are gathered back to the root
    (rank 0) task.

    The SPMD paradigm means there are many copies of the program running
    and each copy runs the same instructions, so programs must be
    written with this in mind. While all copies build the scene to
    render, only one copy (the MPI process with rank 0) actually
    receives all the render results and calls the update function. This
    means that after an observe, only the rank 0 process contains the
    results of the call to the render function for all tasks, and so any
    post processing (including plotting or saving images) should only be
    done on the rank 0 process.

    Also, any further renders which depend on the results of a previous
    render (for example, using adapive samplers) will require
    communication of the results from rank 0 to all the other processes:

        >>> pipeline = RGBPipeline2d()
        >>> camera.pipelines = [pipeline]
        >>> camera.sampler = RGBAdaptivesampler2d(pipeline)
        >>> camera.observe()
        >>> # Update the sampler in each worker process with the results
        >>> # of the previous render for the next pass.
        >>> root_pipeline = camera.render_engine.comm.bcast(pipeline, root=0)
        >>> camera.sampler.pipeline = root_pipeline
        >>> # Now subsequent observes will have the correct statistics.
        >>> camera.observe()

    It is the end user's responsibility to ensure that the number of
    workers is configured appropriately, and this will strongly depend
    on the environment in which the program is launched. Examples of
    running the hybrid engine for 3 popular schedulers are given here.

    Slurm:

        $ sbatch -n <NMPI> -c <NSUB> <script.sh>
        $ # Within script.sh:
        $ mpirun -n <NMPI> --bind-to none <application.py>

        >>> # Within application.py:
        >>> camera.render_engine = HybridEngine(MulticoreEngine(<NSUB>)

    PSB/Torque:

        $ qsub -l nodes=<NMPI>:ppn=<NSUB> <script.sh>
        $ # Within script.sh:
        $ mpirun -n <NMPI> --map-by node --bind-to none <application.py>

        >>> # Within application.py:
        >>> camera.render_engine = HybridEngine(MulticoreEngine(<NSUB>)

    Grid engine is more complicated as there is no portable way to
    specify the number of slots per node, though there is an environment
    variable for the number of separate nodes the job is being run on:

        $ qsub -pe <mpi parallel environment name> <NSLOTS> <script.sh>
        $ # Within script.sh:
        $ mpirun -n $NHOSTS --map-by node --bind-to none <application.py>

        >>> # Within application.py:
        >>> # Find out how many slots are allocated this MPI process's node.
        >>> import os, platform, subprocess
        >>> host = platform.node()
        >>> qstat = subprocess.run(['qstat', '-g', 't'], capture_output=True,
                                   encoding='UTF-8')
        >>> job_id = os.getenv("JOB_ID")
        >>> # Loop through qstat output until we find our job id on this node.
        >>> current_job = ""
        >>> NSUB = 0
        >>> for line in qstat.stdout.splitlines():
        >>>     fields = line.strip().split()
        >>>     if len(fields) >= 9:  # Start of a node's entry
        >>>         current_job = fields[0]
        >>>         current_queue = fields[7]
        >>>         slot_type = fields[8]
        >>>     elif len(fields) >= 2:  # Additional lines of a node's entry
        >>>         current_queue = fields[0]
        >>>         slot_type = fields[1]
        >>>     else:  # Other decorative lines in the output are irrelevant.
        >>>         continue
        >>>     if current_job == job_id and host in current_queue and slot_type == "SLAVE":
        >>>         NSUB += 1
        >>>
        >>> camera.render_engine = HybridEngine(MulticoreEngine(NSUB))

    This class contains some attributes relevant to the MPI environment.
    The sub-engine attributes are accessible through the sub-engine
    directly.

    :ivar comm: the MPI communicator (``MPI_COMM_WORLD``).
    :ivar rank: the mpi rank in the communicator. Only rank 0 contains the
                full results after a call ``to RenderEngine.run()``.
    :ivar nmpi: the number of MPI processes.
    :ivar name: the MPI processor name.
    :ivar totalsize: Total number of workers summed over all MPI processes.
    """
    def __init__(self, subengine=None):
        if not HAVE_MPI:
            raise RuntimeError("The mpi4py package is required to use this engine.")
        if subengine is None:
            subengine = SerialEngine()
        self.subengine = subengine
        comm = MPI.COMM_WORLD
        self.comm = comm
        self.rank = comm.Get_rank()
        self.nmpi = comm.Get_size()
        self.name = MPI.Get_processor_name()
        _local_size = subengine.worker_count()
        self._all_sizes = comm.allgather(_local_size)
        self.total_size = sum(self._all_sizes)

    def run(self, tasks, render, update,
            render_args=(), render_kwargs=None,
            update_args=(), update_kwargs=None):
        # Avoid mutable default arguments.
        if render_kwargs is None:
            render_kwargs = {}
        if update_kwargs is None:
            update_kwargs = {}
        # All processes must have the same tasks, in the same order, so that
        # all processes agree on which subset of tasks each is to work on.
        tasks = self.comm.bcast(tasks, root=0)
        ntasks = len(tasks)
        # Split tasks evenly per individual worker, as different MPI processes
        # may have different numbers of workers. Then re-group the tasks into
        # batches for each MPI worker.
        nworkers = self.total_size
        worker_tasks = [[] for _ in range(nworkers)]
        for i, task in enumerate(tasks):
            worker_tasks[i % nworkers].append(task)
        rank_tasks = []
        idx = 0
        for rank in range(self.nmpi):
            size = self._all_sizes[rank]
            rank_tasks.append(sum(worker_tasks[idx:idx+size], []))
            idx += size
        # On rank 0, spawn a separate process to do the rendering while gathering
        # results in the main thread. We can't use mpi4py to send results from rank
        # 0 to rank 0: here be segfaults. So use a multiprocessing queue instead.
        # This does mean we have to gather data differently on rank 0.
        # We use the 'fork' context here for memory efficiency, which makes this
        # implementation unix-only (and even then, dodgy on MacOS).
        if self.rank == 0:
            ctx = get_context('fork')
            queue = ctx.SimpleQueue()
            worker = ctx.Process(
                target=self.subengine.run,
                kwargs=dict(
                    tasks=rank_tasks[self.rank],
                    render=render, render_args=render_args, render_kwargs=render_kwargs,
                    update=queue.put, update_args=(), update_kwargs={},
                )
            )
            worker.start()
            remaining = ntasks
            local_remaining = len(rank_tasks[self.rank])
            remote_remaining = remaining - local_remaining
            while remaining:
                # Data may come in either from the local queue or from remote MPI.
                if local_remaining and not queue.empty():
                    result = queue.get()
                    local_remaining -= 1
                elif remote_remaining:
                    result = self.comm.recv()
                    remote_remaining -= 1
                else:  # Nothing local or remote yet. Try again.
                    continue
                if isinstance(result, Exception):
                    raise result
                update(result, *update_args, **update_kwargs)
                remaining = local_remaining + remote_remaining
            worker.join()
        else:
            # All other ranks render the subset of tasks assigned to them and
            # send the results through MPI to rank 0 for collating.
            self.subengine.run(
                tasks=rank_tasks[self.rank],
                render=render, render_args=render_args, render_kwargs=render_kwargs,
                update=self.comm.send, update_args=(0,), update_kwargs={},
            )
        self.comm.Barrier()

    def worker_count(self):
        return self.totalsize

    @staticmethod
    def estimate_subworker_count(scheduler=None):
        """
        Estimate the number of processes the sub-worker should use.

        This routine should be called in processes launched from
        job schedulers. It will use environment variables exported
        by the job scheduler to work out the degree of parallelism
        available to the sub-worker.

        Currently, Slurm and Grid Engine are supported. For other
        schedulers it is the user's responsibility to work out (or hard
        code) the sub-worker count.

        :param scheduler: the name of the scheduler in use.
            One of ("slurm", "gridengine")
        :return: The number of sub-workers available in this MPI task.
        """
        if scheduler is None:
            # Try to work out which scheduler is in use.
            if "SLURM_JOB_ID" in os.environ:
                scheduler = "slurm"
            elif "SGE_ROOT" in os.environ:
                scheduler = "gridengine"
            else:
                raise RuntimeError("Can't find a supported scheduler.")
        scheduler = scheduler.lower()
        if scheduler == "slurm":
            nsub = os.getenv("SLURM_CPUS_PER_TASK", 1)
        elif scheduler == "gridengine":
            # Parse qstat output to get the number of SLAVE slots
            # allocated to this node.
            node = platform.node()
            qstat = subprocess.run(['qstat', '-g', 't'], capture_output=True,
                                   encoding='UTF-8')
            job_id = os.environ["JOB_ID"]
            # Loop through qstat output until we find our job id on this node.
            current_job = ""
            nsub = 0
            for line in qstat.stdout.splitlines():
                fields = line.strip().split()
                if len(fields) >= 9:  # Start of a node's entry
                    current_job = fields[0]
                    current_queue = fields[7]
                    slot_type = fields[8]
                elif len(fields) >= 2:  # Additional lines of a node's entry
                    current_queue = fields[0]
                    slot_type = fields[1]
                else:  # Other decorative lines in the output are irrelevant.
                    continue
                if current_job == job_id and node in current_queue and slot_type == "SLAVE":
                    nsub += 1
        else:
            raise RuntimeError(f"{scheduler} is not supported by this function.")
        return nsub



if __name__ == '__main__':

    class Job:

        def __init__(self, engine=None):
            self.total = 0
            self.engine = engine if engine else MulticoreEngine()

        def run(self, v):
            self.total = 0
            self.engine.run(list(range(v)), self.render, self.update, render_args=(10000,))
            return self.total

        def render(self, task, count):
            sum = 0
            for i in range(count):
                sum += 1 / count
            return sum

        def update(self, result):
            self.total += result

    n = 20000

    t = time.time()
    j = Job(SerialEngine())
    print(j.run(n), time.time() - t)

    t = time.time()
    j = Job(MulticoreEngine())
    print(j.run(n), time.time() - t)
